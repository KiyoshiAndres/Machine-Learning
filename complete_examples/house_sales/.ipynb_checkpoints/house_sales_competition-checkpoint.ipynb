{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3c442ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "227b3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/kiyoshitakeuchi/Desktop/Machine Learning/house-prices-advanced-regression-techniques/\"\n",
    "file_name = \"train.csv\"\n",
    "test_file_name = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c87bcd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>14115</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10084</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10382</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>350</td>\n",
       "      <td>11</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>190</td>\n",
       "      <td>RL</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7420</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>118000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "5   6          50       RL         85.0    14115   Pave   NaN      IR1   \n",
       "6   7          20       RL         75.0    10084   Pave   NaN      Reg   \n",
       "7   8          60       RL          NaN    10382   Pave   NaN      IR1   \n",
       "8   9          50       RM         51.0     6120   Pave   NaN      Reg   \n",
       "9  10         190       RL         50.0     7420   Pave   NaN      Reg   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "3         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "4         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "5         Lvl    AllPub  ...        0    NaN  MnPrv        Shed     700   \n",
       "6         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "7         Lvl    AllPub  ...        0    NaN    NaN        Shed     350   \n",
       "8         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "9         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "5     10   2009        WD         Normal     143000  \n",
       "6      8   2007        WD         Normal     307000  \n",
       "7     11   2009        WD         Normal     200000  \n",
       "8      4   2008        WD        Abnorml     129900  \n",
       "9      1   2008        WD         Normal     118000  \n",
       "\n",
       "[10 rows x 81 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.read_csv(file_path + file_name)\n",
    "train_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b74bf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1466</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1467</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7980</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1468</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>63.0</td>\n",
       "      <td>8402</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1469</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>10176</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1470</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8400</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0  1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
       "1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
       "2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
       "3  1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
       "4  1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
       "5  1466          60       RL         75.0    10000   Pave   NaN      IR1   \n",
       "6  1467          20       RL          NaN     7980   Pave   NaN      IR1   \n",
       "7  1468          60       RL         63.0     8402   Pave   NaN      IR1   \n",
       "8  1469          20       RL         85.0    10176   Pave   NaN      Reg   \n",
       "9  1470          20       RL         70.0     8400   Pave   NaN      Reg   \n",
       "\n",
       "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
       "0         Lvl    AllPub  ...         120        0    NaN  MnPrv         NaN   \n",
       "1         Lvl    AllPub  ...           0        0    NaN    NaN        Gar2   \n",
       "2         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n",
       "3         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "4         HLS    AllPub  ...         144        0    NaN    NaN         NaN   \n",
       "5         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "6         Lvl    AllPub  ...           0        0    NaN  GdPrv        Shed   \n",
       "7         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "8         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "9         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "0       0      6    2010        WD         Normal  \n",
       "1   12500      6    2010        WD         Normal  \n",
       "2       0      3    2010        WD         Normal  \n",
       "3       0      6    2010        WD         Normal  \n",
       "4       0      1    2010        WD         Normal  \n",
       "5       0      4    2010        WD         Normal  \n",
       "6     500      3    2010        WD         Normal  \n",
       "7       0      5    2010        WD         Normal  \n",
       "8       0      2    2010        WD         Normal  \n",
       "9       0      4    2010        WD         Normal  \n",
       "\n",
       "[10 rows x 80 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = \"SalePrice\"\n",
    "test_set = pd.read_csv(file_path + test_file_name)\n",
    "test_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "566c225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(data_frame): \n",
    "    mean = data_frame.select_dtypes(include = 'number').median()\n",
    "    mean = mean.to_dict()\n",
    "    data_frame.fillna(value=mean, inplace=True)\n",
    "    data_frame.fillna(value=\"?\", inplace=True)\n",
    "    return\n",
    "\n",
    "remove_nan(test_set)\n",
    "remove_nan(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8808dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_set.copy()\n",
    "labels = features.pop(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36774eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_frame):    \n",
    "    inputs = {}\n",
    "\n",
    "    for name, column in data_frame.items():\n",
    "        dtype = column.dtype\n",
    "        if dtype == object:\n",
    "            dtype = tf.string\n",
    "        else:\n",
    "            dtype = tf.float32\n",
    "        inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n",
    "        \n",
    "    numeric_inputs = {name:input for name,input in inputs.items() \n",
    "                      if input.dtype==tf.float32}\n",
    "\n",
    "\n",
    "    x = layers.Concatenate()(list(numeric_inputs.values()))\n",
    "    norm = layers.Normalization()\n",
    "    norm.adapt(np.array(train_set[numeric_inputs.keys()]))\n",
    "    all_numeric_inputs = norm(x)\n",
    "\n",
    "    preprocessed_inputs = [all_numeric_inputs]\n",
    "    \n",
    "    for name, input in inputs.items():\n",
    "        if input.dtype == tf.float32:\n",
    "            continue \n",
    "\n",
    "        lookup = layers.StringLookup(vocabulary=np.unique(data_frame[name]))\n",
    "        one_hot = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
    "        x = lookup(input)\n",
    "        x = one_hot(x)\n",
    "        preprocessed_inputs.append(x)\n",
    "    \n",
    "    preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
    "\n",
    "    preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat)\n",
    "    \n",
    "    return [inputs, preprocessing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "decfeb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data_frame):\n",
    "    body = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "    \n",
    "    inputs, preprocessing_head = preprocess_data(data_frame)\n",
    "    preprocessed_inputs = preprocessing_head(inputs)\n",
    "    result = body(preprocessed_inputs)\n",
    "    model = tf.keras.Model(inputs, result)\n",
    "    \n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dffbfcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {name: np.array(value) \n",
    "                         for name, value in features.items()}\n",
    "test_features_dict = {name: np.array(value) \n",
    "                         for name, value in test_set.copy().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93baddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6439623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 180585.0625 - val_loss: 182228.7188\n",
      "Epoch 2/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 180539.6719 - val_loss: 182137.3906\n",
      "Epoch 3/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 180358.0781 - val_loss: 181846.1562\n",
      "Epoch 4/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 179884.3281 - val_loss: 181177.2656\n",
      "Epoch 5/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 178906.8594 - val_loss: 179913.0000\n",
      "Epoch 6/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 177211.1562 - val_loss: 177867.9844\n",
      "Epoch 7/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 174612.9219 - val_loss: 174868.5469\n",
      "Epoch 8/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 170928.6562 - val_loss: 170734.3906\n",
      "Epoch 9/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 165970.3438 - val_loss: 165296.0312\n",
      "Epoch 10/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 159579.7188 - val_loss: 158405.7812\n",
      "Epoch 11/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 151596.7812 - val_loss: 149921.3281\n",
      "Epoch 12/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 141884.9531 - val_loss: 139715.0156\n",
      "Epoch 13/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 130415.7656 - val_loss: 127674.5312\n",
      "Epoch 14/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 117058.3828 - val_loss: 113990.3438\n",
      "Epoch 15/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 101983.1641 - val_loss: 98809.6172\n",
      "Epoch 16/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 86303.1172 - val_loss: 83265.9688\n",
      "Epoch 17/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 71489.1875 - val_loss: 68597.1875\n",
      "Epoch 18/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 59203.8477 - val_loss: 57227.7344\n",
      "Epoch 19/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 50772.7461 - val_loss: 49840.1250\n",
      "Epoch 20/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 45498.4688 - val_loss: 45114.4219\n",
      "Epoch 21/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 41604.3398 - val_loss: 41659.7344\n",
      "Epoch 22/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 38597.9922 - val_loss: 38957.1797\n",
      "Epoch 23/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 36096.6016 - val_loss: 36713.2656\n",
      "Epoch 24/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 33862.7969 - val_loss: 34806.6172\n",
      "Epoch 25/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 31915.6953 - val_loss: 33207.2891\n",
      "Epoch 26/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 30185.6035 - val_loss: 31825.6133\n",
      "Epoch 27/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 28678.1719 - val_loss: 30661.9785\n",
      "Epoch 28/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 27426.1309 - val_loss: 29752.6992\n",
      "Epoch 29/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 26349.8535 - val_loss: 28781.8398\n",
      "Epoch 30/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 25485.3965 - val_loss: 28167.2070\n",
      "Epoch 31/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 24741.5859 - val_loss: 27687.8945\n",
      "Epoch 32/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 24126.3066 - val_loss: 27205.4883\n",
      "Epoch 33/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 23661.5059 - val_loss: 26839.0371\n",
      "Epoch 34/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 23232.8828 - val_loss: 26532.0352\n",
      "Epoch 35/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 22846.9805 - val_loss: 26169.9648\n",
      "Epoch 36/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 22498.0586 - val_loss: 25865.8223\n",
      "Epoch 37/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 22167.8945 - val_loss: 25494.4258\n",
      "Epoch 38/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 21870.3574 - val_loss: 25281.8008\n",
      "Epoch 39/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 21574.3828 - val_loss: 25065.9238\n",
      "Epoch 40/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 21326.4160 - val_loss: 24754.3535\n",
      "Epoch 41/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 21071.9004 - val_loss: 24590.8320\n",
      "Epoch 42/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 20841.0430 - val_loss: 24352.9551\n",
      "Epoch 43/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 20616.3809 - val_loss: 24178.2422\n",
      "Epoch 44/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 20403.0332 - val_loss: 23947.3184\n",
      "Epoch 45/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 20188.1738 - val_loss: 23864.0781\n",
      "Epoch 46/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 19983.3184 - val_loss: 23549.8848\n",
      "Epoch 47/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 19782.4883 - val_loss: 23367.0469\n",
      "Epoch 48/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 19584.3926 - val_loss: 23279.4766\n",
      "Epoch 49/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 19420.1582 - val_loss: 23117.1387\n",
      "Epoch 50/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 19254.6523 - val_loss: 22897.5898\n",
      "Epoch 51/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 19080.9961 - val_loss: 22876.0078\n",
      "Epoch 52/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 18942.1523 - val_loss: 22663.7676\n",
      "Epoch 53/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 18813.6699 - val_loss: 22607.0059\n",
      "Epoch 54/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 18653.4141 - val_loss: 22389.3926\n",
      "Epoch 55/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 18507.0645 - val_loss: 22335.6680\n",
      "Epoch 56/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 18371.1465 - val_loss: 22230.4453\n",
      "Epoch 57/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 18243.6426 - val_loss: 22189.2988\n",
      "Epoch 58/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 18115.5293 - val_loss: 21977.2422\n",
      "Epoch 59/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 17991.6465 - val_loss: 21913.6406\n",
      "Epoch 60/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 17854.5352 - val_loss: 21783.9570\n",
      "Epoch 61/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 17731.5234 - val_loss: 21724.9043\n",
      "Epoch 62/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 17614.8340 - val_loss: 21588.0078\n",
      "Epoch 63/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 17504.5059 - val_loss: 21557.3984\n",
      "Epoch 64/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 17387.7969 - val_loss: 21478.0723\n",
      "Epoch 65/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 17278.4922 - val_loss: 21365.6094\n",
      "Epoch 66/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 17170.1367 - val_loss: 21264.5977\n",
      "Epoch 67/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 17065.9316 - val_loss: 21180.3965\n",
      "Epoch 68/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16974.0703 - val_loss: 21091.4531\n",
      "Epoch 69/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16881.2480 - val_loss: 21038.7598\n",
      "Epoch 70/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16781.9844 - val_loss: 20951.3105\n",
      "Epoch 71/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16688.1973 - val_loss: 20873.8906\n",
      "Epoch 72/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16605.0430 - val_loss: 20781.9180\n",
      "Epoch 73/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16529.9219 - val_loss: 20711.3945\n",
      "Epoch 74/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16443.3496 - val_loss: 20663.6289\n",
      "Epoch 75/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16349.5410 - val_loss: 20598.8398\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 3ms/step - loss: 16275.7812 - val_loss: 20604.7148\n",
      "Epoch 77/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16216.9111 - val_loss: 20483.0098\n",
      "Epoch 78/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16128.1338 - val_loss: 20461.6523\n",
      "Epoch 79/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16073.8301 - val_loss: 20389.9473\n",
      "Epoch 80/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15996.4707 - val_loss: 20340.2910\n",
      "Epoch 81/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15939.7432 - val_loss: 20254.7148\n",
      "Epoch 82/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15904.0615 - val_loss: 20223.2656\n",
      "Epoch 83/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15840.8291 - val_loss: 20159.4492\n",
      "Epoch 84/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15750.2461 - val_loss: 20127.2734\n",
      "Epoch 85/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15674.1562 - val_loss: 20070.6797\n",
      "Epoch 86/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15620.6250 - val_loss: 20042.3340\n",
      "Epoch 87/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15570.9277 - val_loss: 19990.2812\n",
      "Epoch 88/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15504.5176 - val_loss: 19939.7246\n",
      "Epoch 89/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15455.9932 - val_loss: 19912.2617\n",
      "Epoch 90/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15407.3389 - val_loss: 19865.2227\n",
      "Epoch 91/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15366.9434 - val_loss: 19824.4141\n",
      "Epoch 92/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15320.7539 - val_loss: 19792.6309\n",
      "Epoch 93/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15273.2568 - val_loss: 19747.4629\n",
      "Epoch 94/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15225.4141 - val_loss: 19717.0215\n",
      "Epoch 95/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15166.9248 - val_loss: 19706.1738\n",
      "Epoch 96/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15138.3115 - val_loss: 19656.3594\n",
      "Epoch 97/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15092.8115 - val_loss: 19622.9395\n",
      "Epoch 98/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15052.1816 - val_loss: 19586.6465\n",
      "Epoch 99/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15026.9111 - val_loss: 19569.9258\n",
      "Epoch 100/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14975.8291 - val_loss: 19540.5469\n",
      "Epoch 101/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14935.6914 - val_loss: 19497.6738\n",
      "Epoch 102/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14912.0176 - val_loss: 19473.2148\n",
      "Epoch 103/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14863.8271 - val_loss: 19449.2480\n",
      "Epoch 104/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14852.6748 - val_loss: 19421.0645\n",
      "Epoch 105/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14830.4473 - val_loss: 19386.2402\n",
      "Epoch 106/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14778.0381 - val_loss: 19360.8945\n",
      "Epoch 107/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14742.1143 - val_loss: 19347.8711\n",
      "Epoch 108/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14711.7412 - val_loss: 19317.7168\n",
      "Epoch 109/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14673.3389 - val_loss: 19289.2344\n",
      "Epoch 110/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14645.4482 - val_loss: 19267.0449\n",
      "Epoch 111/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14614.2959 - val_loss: 19244.9277\n",
      "Epoch 112/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14586.4062 - val_loss: 19261.1953\n",
      "Epoch 113/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14577.4951 - val_loss: 19212.1211\n",
      "Epoch 114/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14535.3320 - val_loss: 19199.8789\n",
      "Epoch 115/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14528.7314 - val_loss: 19183.7695\n",
      "Epoch 116/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14472.8574 - val_loss: 19157.0898\n",
      "Epoch 117/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14452.6543 - val_loss: 19138.6289\n",
      "Epoch 118/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14442.6182 - val_loss: 19203.5547\n",
      "Epoch 119/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14450.5273 - val_loss: 19145.9531\n",
      "Epoch 120/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14387.2246 - val_loss: 19104.3047\n",
      "Epoch 121/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14351.5586 - val_loss: 19132.3184\n",
      "Epoch 122/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14359.2744 - val_loss: 19075.6348\n",
      "Epoch 123/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14296.6572 - val_loss: 19029.4355\n",
      "Epoch 124/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14282.7246 - val_loss: 19048.6211\n",
      "Epoch 125/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14257.5986 - val_loss: 19015.7930\n",
      "Epoch 126/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14227.7832 - val_loss: 19010.0977\n",
      "Epoch 127/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14220.9473 - val_loss: 18978.3535\n",
      "Epoch 128/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14208.1094 - val_loss: 18996.2910\n",
      "Epoch 129/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14155.6572 - val_loss: 18964.3262\n",
      "Epoch 130/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14124.2861 - val_loss: 18948.2227\n",
      "Epoch 131/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14116.7900 - val_loss: 18933.5625\n",
      "Epoch 132/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14102.2754 - val_loss: 18958.3848\n",
      "Epoch 133/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14072.6621 - val_loss: 18912.1250\n",
      "Epoch 134/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14049.5068 - val_loss: 18930.6309\n",
      "Epoch 135/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14034.8838 - val_loss: 18930.6328\n",
      "Epoch 136/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14042.8125 - val_loss: 18867.3574\n",
      "Epoch 137/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13992.8926 - val_loss: 18865.5840\n",
      "Epoch 138/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13985.4482 - val_loss: 18856.9199\n",
      "Epoch 139/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13951.1543 - val_loss: 18825.7246\n",
      "Epoch 140/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13944.9844 - val_loss: 18808.5098\n",
      "Epoch 141/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13924.7881 - val_loss: 18815.7324\n",
      "Epoch 142/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13905.7686 - val_loss: 18770.0605\n",
      "Epoch 143/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13918.8311 - val_loss: 18759.5605\n",
      "Epoch 144/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13880.8418 - val_loss: 18756.1348\n",
      "Epoch 145/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13845.3516 - val_loss: 18828.1738\n",
      "Epoch 146/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13858.3818 - val_loss: 18705.9277\n",
      "Epoch 147/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13819.4043 - val_loss: 18746.4238\n",
      "Epoch 148/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13812.9434 - val_loss: 18701.7402\n",
      "Epoch 149/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13798.9287 - val_loss: 18700.0488\n",
      "Epoch 150/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13797.9619 - val_loss: 18769.1875\n",
      "Epoch 151/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 3ms/step - loss: 13789.2959 - val_loss: 18685.3301\n",
      "Epoch 152/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13757.2920 - val_loss: 18652.7637\n",
      "Epoch 153/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13748.7695 - val_loss: 18640.1738\n",
      "Epoch 154/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13738.3408 - val_loss: 18654.6309\n",
      "Epoch 155/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13736.2998 - val_loss: 18632.0195\n",
      "Epoch 156/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13771.0010 - val_loss: 18631.0195\n",
      "Epoch 157/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13711.7725 - val_loss: 18622.2832\n",
      "Epoch 158/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13696.6182 - val_loss: 18614.9258\n",
      "Epoch 159/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13666.0020 - val_loss: 18586.7734\n",
      "Epoch 160/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13667.9834 - val_loss: 18617.1562\n",
      "Epoch 161/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13654.2979 - val_loss: 18619.8613\n",
      "Epoch 162/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13645.8213 - val_loss: 18647.7285\n",
      "Epoch 163/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13729.4111 - val_loss: 18641.8379\n",
      "Epoch 164/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13619.0176 - val_loss: 18593.4824\n",
      "Epoch 165/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13637.8057 - val_loss: 18609.6562\n",
      "Epoch 166/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13614.1465 - val_loss: 18539.3203\n",
      "Epoch 167/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13611.2949 - val_loss: 18597.1445\n",
      "Epoch 168/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13599.2598 - val_loss: 18551.7891\n",
      "Epoch 169/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13598.3711 - val_loss: 18539.2734\n",
      "Epoch 170/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13556.1250 - val_loss: 18511.4668\n",
      "Epoch 171/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13544.2324 - val_loss: 18509.0840\n",
      "Epoch 172/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13543.9541 - val_loss: 18479.5117\n",
      "Epoch 173/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13523.7236 - val_loss: 18482.2949\n",
      "Epoch 174/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13508.2061 - val_loss: 18454.8242\n",
      "Epoch 175/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13533.7461 - val_loss: 18464.5684\n",
      "Epoch 176/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13504.1748 - val_loss: 18489.4395\n",
      "Epoch 177/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13492.6914 - val_loss: 18429.9219\n",
      "Epoch 178/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13488.8633 - val_loss: 18426.3574\n",
      "Epoch 179/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13452.0566 - val_loss: 18451.1875\n",
      "Epoch 180/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13454.9668 - val_loss: 18423.0312\n",
      "Epoch 181/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13458.3096 - val_loss: 18411.9512\n",
      "Epoch 182/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13445.0938 - val_loss: 18415.0977\n",
      "Epoch 183/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13432.9766 - val_loss: 18446.1973\n",
      "Epoch 184/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13445.3330 - val_loss: 18404.2617\n",
      "Epoch 185/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13409.7441 - val_loss: 18406.8047\n",
      "Epoch 186/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13403.7715 - val_loss: 18395.2461\n",
      "Epoch 187/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13396.6904 - val_loss: 18392.7461\n",
      "Epoch 188/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13387.6299 - val_loss: 18364.0234\n",
      "Epoch 189/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13372.6494 - val_loss: 18368.1582\n",
      "Epoch 190/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13372.7412 - val_loss: 18369.6855\n",
      "Epoch 191/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13366.0654 - val_loss: 18417.9277\n",
      "Epoch 192/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13357.3037 - val_loss: 18407.7422\n",
      "Epoch 193/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13341.7803 - val_loss: 18348.8457\n",
      "Epoch 194/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13343.8320 - val_loss: 18347.2891\n",
      "Epoch 195/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13333.3740 - val_loss: 18406.3086\n",
      "Epoch 196/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13334.7881 - val_loss: 18349.0801\n",
      "Epoch 197/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13318.8721 - val_loss: 18328.0879\n",
      "Epoch 198/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13316.2715 - val_loss: 18311.3184\n",
      "Epoch 199/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13309.6689 - val_loss: 18306.4941\n",
      "Epoch 200/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13307.3857 - val_loss: 18295.9570\n",
      "Epoch 201/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13283.0137 - val_loss: 18290.6758\n",
      "Epoch 202/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13279.4512 - val_loss: 18316.4473\n",
      "Epoch 203/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13258.6211 - val_loss: 18308.1309\n",
      "Epoch 204/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13256.1699 - val_loss: 18294.0488\n",
      "Epoch 205/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13253.7549 - val_loss: 18305.6660\n",
      "Epoch 206/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13252.3906 - val_loss: 18304.3926\n",
      "Epoch 207/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13245.4229 - val_loss: 18267.9258\n",
      "Epoch 208/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13257.3154 - val_loss: 18315.2656\n",
      "Epoch 209/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13304.4199 - val_loss: 18302.2129\n",
      "Epoch 210/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13222.6494 - val_loss: 18298.2129\n",
      "Epoch 211/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13211.1279 - val_loss: 18269.8789\n",
      "Epoch 212/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13199.9639 - val_loss: 18263.8496\n",
      "Epoch 213/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13198.3799 - val_loss: 18249.5742\n",
      "Epoch 214/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13191.5186 - val_loss: 18251.2363\n",
      "Epoch 215/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13204.3936 - val_loss: 18328.1602\n",
      "Epoch 216/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13183.3281 - val_loss: 18226.4512\n",
      "Epoch 217/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13167.8506 - val_loss: 18237.1855\n",
      "Epoch 218/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13170.0332 - val_loss: 18235.9902\n",
      "Epoch 219/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13172.0410 - val_loss: 18198.7969\n",
      "Epoch 220/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13153.9043 - val_loss: 18195.3633\n",
      "Epoch 221/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13179.9297 - val_loss: 18235.0176\n",
      "Epoch 222/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13154.0469 - val_loss: 18196.5625\n",
      "Epoch 223/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13144.3242 - val_loss: 18179.9355\n",
      "Epoch 224/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13164.3848 - val_loss: 18160.4023\n",
      "Epoch 225/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13220.7090 - val_loss: 18172.1875\n",
      "Epoch 226/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 3ms/step - loss: 13163.9951 - val_loss: 18222.5820\n",
      "Epoch 227/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13128.6631 - val_loss: 18160.8242\n",
      "Epoch 228/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13119.5586 - val_loss: 18145.4746\n",
      "Epoch 229/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13118.9258 - val_loss: 18167.2227\n",
      "Epoch 230/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13098.4619 - val_loss: 18139.1582\n",
      "Epoch 231/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13093.1895 - val_loss: 18141.6602\n",
      "Epoch 232/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13085.2334 - val_loss: 18131.5195\n",
      "Epoch 233/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13092.7656 - val_loss: 18124.9258\n",
      "Epoch 234/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13088.7783 - val_loss: 18182.1602\n",
      "Epoch 235/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13119.3887 - val_loss: 18128.5898\n",
      "Epoch 236/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13059.7441 - val_loss: 18108.0762\n",
      "Epoch 237/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13054.5840 - val_loss: 18162.9316\n",
      "Epoch 238/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13095.6006 - val_loss: 18163.6152\n",
      "Epoch 239/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13076.3154 - val_loss: 18091.8477\n",
      "Epoch 240/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13039.8008 - val_loss: 18090.9727\n",
      "Epoch 241/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13113.3066 - val_loss: 18076.3086\n",
      "Epoch 242/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13075.1846 - val_loss: 18155.6348\n",
      "Epoch 243/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13031.6992 - val_loss: 18095.8555\n",
      "Epoch 244/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13034.5264 - val_loss: 18071.5605\n",
      "Epoch 245/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13030.3906 - val_loss: 18055.8945\n",
      "Epoch 246/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13041.7119 - val_loss: 18045.2988\n",
      "Epoch 247/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13033.9023 - val_loss: 18046.9492\n",
      "Epoch 248/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13006.6406 - val_loss: 18065.9238\n",
      "Epoch 249/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13028.4219 - val_loss: 18056.1465\n",
      "Epoch 250/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13010.9961 - val_loss: 18056.8965\n",
      "Epoch 251/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12992.1514 - val_loss: 18082.7422\n",
      "Epoch 252/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13007.6484 - val_loss: 18040.8828\n",
      "Epoch 253/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12980.7812 - val_loss: 18066.8730\n",
      "Epoch 254/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12983.8613 - val_loss: 18034.5664\n",
      "Epoch 255/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12975.5996 - val_loss: 18012.8398\n",
      "Epoch 256/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12990.5869 - val_loss: 18039.9141\n",
      "Epoch 257/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12999.4160 - val_loss: 18049.3555\n",
      "Epoch 258/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12976.9521 - val_loss: 18060.6465\n",
      "Epoch 259/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12972.9531 - val_loss: 18063.4629\n",
      "Epoch 260/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13011.5234 - val_loss: 18102.5547\n",
      "Epoch 261/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12974.7002 - val_loss: 18038.7852\n",
      "Epoch 262/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12930.8535 - val_loss: 18002.6914\n",
      "Epoch 263/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12956.7471 - val_loss: 18049.6641\n",
      "Epoch 264/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12934.1006 - val_loss: 18004.9688\n",
      "Epoch 265/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12938.0205 - val_loss: 18003.7363\n",
      "Epoch 266/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12959.1182 - val_loss: 18009.8027\n",
      "Epoch 267/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12920.3877 - val_loss: 17994.9902\n",
      "Epoch 268/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12929.1592 - val_loss: 17990.1328\n",
      "Epoch 269/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12917.6289 - val_loss: 17992.8613\n",
      "Epoch 270/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12903.4863 - val_loss: 17995.9570\n",
      "Epoch 271/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12913.0732 - val_loss: 17994.6465\n",
      "Epoch 272/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12914.0449 - val_loss: 17969.9219\n",
      "Epoch 273/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12895.2080 - val_loss: 17979.8555\n",
      "Epoch 274/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12918.7090 - val_loss: 17955.5918\n",
      "Epoch 275/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12903.8047 - val_loss: 17993.0312\n",
      "Epoch 276/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12893.8369 - val_loss: 17986.8867\n",
      "Epoch 277/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12890.5898 - val_loss: 17970.2109\n",
      "Epoch 278/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12888.1045 - val_loss: 17972.3047\n",
      "Epoch 279/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12881.9482 - val_loss: 17973.0449\n",
      "Epoch 280/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12869.5312 - val_loss: 17971.8535\n",
      "Epoch 281/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12859.2676 - val_loss: 17956.8574\n",
      "Epoch 282/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12876.5215 - val_loss: 17953.2734\n",
      "Epoch 283/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12866.1465 - val_loss: 17971.5273\n",
      "Epoch 284/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12866.7578 - val_loss: 17926.8965\n",
      "Epoch 285/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12881.7256 - val_loss: 17917.1934\n",
      "Epoch 286/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12874.4150 - val_loss: 17926.7402\n",
      "Epoch 287/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12847.3994 - val_loss: 17959.2559\n",
      "Epoch 288/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12857.3887 - val_loss: 17940.4746\n",
      "Epoch 289/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12854.9336 - val_loss: 17974.0078\n",
      "Epoch 290/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12837.7881 - val_loss: 17943.8320\n",
      "Epoch 291/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12841.8174 - val_loss: 17916.2793\n",
      "Epoch 292/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12838.7363 - val_loss: 17964.5586\n",
      "Epoch 293/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12821.9238 - val_loss: 17941.5625\n",
      "Epoch 294/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12830.0635 - val_loss: 17973.9551\n",
      "Epoch 295/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12820.3760 - val_loss: 17978.5469\n",
      "Epoch 296/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12855.4141 - val_loss: 17954.4980\n",
      "Epoch 297/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12813.6006 - val_loss: 17998.4648\n",
      "Epoch 298/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12852.5566 - val_loss: 17942.3770\n",
      "Epoch 299/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12814.0195 - val_loss: 17934.7109\n",
      "Epoch 300/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12801.8018 - val_loss: 17905.0449\n",
      "Epoch 301/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 3ms/step - loss: 12815.4502 - val_loss: 17899.7656\n",
      "Epoch 302/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12817.4287 - val_loss: 17926.6602\n",
      "Epoch 303/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12808.5352 - val_loss: 17895.0762\n",
      "Epoch 304/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12803.5498 - val_loss: 17934.9629\n",
      "Epoch 305/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12796.8584 - val_loss: 17953.6992\n",
      "Epoch 306/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12795.9062 - val_loss: 17944.5801\n",
      "Epoch 307/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12796.3105 - val_loss: 17979.4414\n",
      "Epoch 308/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12803.8643 - val_loss: 17907.5625\n",
      "Epoch 309/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12786.6846 - val_loss: 17922.5215\n",
      "Epoch 310/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12790.6826 - val_loss: 17893.8477\n",
      "Epoch 311/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12774.7920 - val_loss: 17890.2520\n",
      "Epoch 312/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12762.8193 - val_loss: 17897.0391\n",
      "Epoch 313/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12774.2256 - val_loss: 17921.3809\n",
      "Epoch 314/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12755.3994 - val_loss: 17954.8184\n",
      "Epoch 315/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12786.2783 - val_loss: 17914.7246\n",
      "Epoch 316/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12756.9326 - val_loss: 17906.1348\n",
      "Epoch 317/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12749.7314 - val_loss: 17907.4102\n",
      "Epoch 318/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12744.2920 - val_loss: 17905.8652\n",
      "Epoch 319/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12740.8252 - val_loss: 17942.0605\n",
      "Epoch 320/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12739.7314 - val_loss: 17907.5137\n",
      "Epoch 321/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12756.7539 - val_loss: 17895.1641\n",
      "Epoch 322/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12730.6807 - val_loss: 17884.7910\n",
      "Epoch 323/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12742.7715 - val_loss: 17904.0215\n",
      "Epoch 324/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12734.1543 - val_loss: 17899.3184\n",
      "Epoch 325/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12724.2334 - val_loss: 17908.4707\n",
      "Epoch 326/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12714.7773 - val_loss: 17895.2266\n",
      "Epoch 327/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12699.7969 - val_loss: 17939.8125\n",
      "Epoch 328/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12714.9824 - val_loss: 17899.0059\n",
      "Epoch 329/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12711.2236 - val_loss: 17861.4805\n",
      "Epoch 330/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12714.7070 - val_loss: 17886.9766\n",
      "Epoch 331/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12700.5186 - val_loss: 17856.7383\n",
      "Epoch 332/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12699.3242 - val_loss: 17891.5449\n",
      "Epoch 333/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12686.0107 - val_loss: 17867.4082\n",
      "Epoch 334/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12686.0898 - val_loss: 17841.2012\n",
      "Epoch 335/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12695.3643 - val_loss: 17902.4766\n",
      "Epoch 336/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12711.3672 - val_loss: 17884.9746\n",
      "Epoch 337/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12687.1738 - val_loss: 17907.5195\n",
      "Epoch 338/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12686.9658 - val_loss: 17937.1406\n",
      "Epoch 339/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12667.8730 - val_loss: 17906.9863\n",
      "Epoch 340/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12668.3643 - val_loss: 17851.5605\n",
      "Epoch 341/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12665.7578 - val_loss: 17902.8203\n",
      "Epoch 342/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12661.0625 - val_loss: 17865.0898\n",
      "Epoch 343/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12646.3916 - val_loss: 17887.5723\n",
      "Epoch 344/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12696.9434 - val_loss: 17844.6191\n",
      "Epoch 345/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12647.8008 - val_loss: 17864.5625\n",
      "Epoch 346/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12640.5576 - val_loss: 17859.5898\n",
      "Epoch 347/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12641.3975 - val_loss: 17914.0020\n",
      "Epoch 348/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12637.0996 - val_loss: 17868.6953\n",
      "Epoch 349/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12625.1836 - val_loss: 17889.0781\n",
      "Epoch 350/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12644.5742 - val_loss: 17919.8691\n",
      "Epoch 351/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12666.1846 - val_loss: 17865.0391\n",
      "Epoch 352/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12627.6504 - val_loss: 17965.4141\n",
      "Epoch 353/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12642.7148 - val_loss: 17849.1523\n",
      "Epoch 354/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12610.3848 - val_loss: 17831.2090\n",
      "Epoch 355/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12602.1074 - val_loss: 17857.9766\n",
      "Epoch 356/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12598.9756 - val_loss: 17821.5195\n",
      "Epoch 357/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12597.8594 - val_loss: 17829.2949\n",
      "Epoch 358/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12590.0020 - val_loss: 17852.6895\n",
      "Epoch 359/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12594.9150 - val_loss: 17832.8984\n",
      "Epoch 360/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12587.6523 - val_loss: 17839.1367\n",
      "Epoch 361/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12586.1211 - val_loss: 17786.9805\n",
      "Epoch 362/500\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 12617.9414 - val_loss: 17809.4414\n",
      "Epoch 363/500\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 12580.3584 - val_loss: 17840.8086\n",
      "Epoch 364/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12581.6680 - val_loss: 17833.8066\n",
      "Epoch 365/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12574.6094 - val_loss: 17827.8398\n",
      "Epoch 366/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12560.6631 - val_loss: 17786.3535\n",
      "Epoch 367/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12553.8164 - val_loss: 17820.6875\n",
      "Epoch 368/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12557.3975 - val_loss: 17839.8047\n",
      "Epoch 369/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12558.3115 - val_loss: 17823.0391\n",
      "Epoch 370/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12565.2529 - val_loss: 17818.4844\n",
      "Epoch 371/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12553.4707 - val_loss: 17808.2949\n",
      "Epoch 372/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12538.8027 - val_loss: 17830.0430\n",
      "Epoch 373/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12539.0518 - val_loss: 17822.1621\n",
      "Epoch 374/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12543.5371 - val_loss: 17843.1602\n",
      "Epoch 375/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12568.7656 - val_loss: 17831.0566\n",
      "Epoch 376/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 3ms/step - loss: 12537.1924 - val_loss: 17807.5156\n",
      "Epoch 377/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12572.2793 - val_loss: 17779.4238\n",
      "Epoch 378/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12529.9238 - val_loss: 17806.5566\n",
      "Epoch 379/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12519.3428 - val_loss: 17762.7852\n",
      "Epoch 380/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12529.1465 - val_loss: 17822.7520\n",
      "Epoch 381/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12511.3584 - val_loss: 17849.5566\n",
      "Epoch 382/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12512.2520 - val_loss: 17821.5078\n",
      "Epoch 383/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12534.2422 - val_loss: 17786.9355\n",
      "Epoch 384/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12539.2803 - val_loss: 17804.5781\n",
      "Epoch 385/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12492.5986 - val_loss: 17834.7363\n",
      "Epoch 386/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12504.0840 - val_loss: 17784.0059\n",
      "Epoch 387/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12491.4990 - val_loss: 17781.0488\n",
      "Epoch 388/500\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 12491.5059 - val_loss: 17789.0547\n",
      "Epoch 389/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12481.8506 - val_loss: 17772.3320\n",
      "Epoch 390/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12492.5439 - val_loss: 17751.1562\n",
      "Epoch 391/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12485.7539 - val_loss: 17787.2324\n",
      "Epoch 392/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12467.8770 - val_loss: 17769.2051\n",
      "Epoch 393/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12469.7139 - val_loss: 17808.7891\n",
      "Epoch 394/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12473.4941 - val_loss: 17757.9102\n",
      "Epoch 395/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12470.3818 - val_loss: 17835.0605\n",
      "Epoch 396/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12501.8105 - val_loss: 17804.4082\n",
      "Epoch 397/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12458.4746 - val_loss: 17810.2793\n",
      "Epoch 398/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12449.7754 - val_loss: 17891.6055\n",
      "Epoch 399/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12479.4004 - val_loss: 17742.6270\n",
      "Epoch 400/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12466.2695 - val_loss: 17771.6172\n",
      "Epoch 401/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12436.2402 - val_loss: 17793.1973\n",
      "Epoch 402/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12439.6084 - val_loss: 17813.8887\n",
      "Epoch 403/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12444.6963 - val_loss: 17769.3242\n",
      "Epoch 404/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12433.2764 - val_loss: 17788.2324\n",
      "Epoch 405/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12426.1074 - val_loss: 17780.9297\n",
      "Epoch 406/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12419.1123 - val_loss: 17766.1191\n",
      "Epoch 407/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12434.3018 - val_loss: 17762.3320\n",
      "Epoch 408/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12416.5762 - val_loss: 17747.2656\n",
      "Epoch 409/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12413.9404 - val_loss: 17776.4473\n",
      "Epoch 410/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12406.5547 - val_loss: 17784.5215\n",
      "Epoch 411/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12404.7607 - val_loss: 17780.8008\n",
      "Epoch 412/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12439.5801 - val_loss: 17795.3672\n",
      "Epoch 413/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12433.3574 - val_loss: 17742.1152\n",
      "Epoch 414/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12406.0176 - val_loss: 17714.1797\n",
      "Epoch 415/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12442.5107 - val_loss: 17720.4980\n",
      "Epoch 416/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12413.7539 - val_loss: 17761.0918\n",
      "Epoch 417/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12384.9385 - val_loss: 17752.6465\n",
      "Epoch 418/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12392.6006 - val_loss: 17753.2988\n",
      "Epoch 419/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12390.0742 - val_loss: 17772.5996\n",
      "Epoch 420/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12388.0039 - val_loss: 17826.2305\n",
      "Epoch 421/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12380.6162 - val_loss: 17769.3555\n",
      "Epoch 422/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12377.3906 - val_loss: 17759.8750\n",
      "Epoch 423/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12369.5244 - val_loss: 17763.2148\n",
      "Epoch 424/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12364.1738 - val_loss: 17778.7754\n",
      "Epoch 425/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12362.8848 - val_loss: 17780.3027\n",
      "Epoch 426/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12355.2471 - val_loss: 17752.4355\n",
      "Epoch 427/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12352.0537 - val_loss: 17740.0508\n",
      "Epoch 428/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12359.7422 - val_loss: 17752.9883\n",
      "Epoch 429/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12357.4893 - val_loss: 17741.5977\n",
      "Epoch 430/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12341.0938 - val_loss: 17756.7715\n",
      "Epoch 431/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12335.7061 - val_loss: 17744.8828\n",
      "Epoch 432/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12362.6836 - val_loss: 17754.0430\n",
      "Epoch 433/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12336.4229 - val_loss: 17738.2324\n",
      "Epoch 434/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12327.8398 - val_loss: 17738.2227\n",
      "Epoch 435/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12320.1152 - val_loss: 17777.3105\n",
      "Epoch 436/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12326.1846 - val_loss: 17765.2676\n",
      "Epoch 437/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12318.9961 - val_loss: 17751.5059\n",
      "Epoch 438/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12319.6250 - val_loss: 17698.0762\n",
      "Epoch 439/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12327.9893 - val_loss: 17752.7988\n",
      "Epoch 440/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12313.7656 - val_loss: 17763.6660\n",
      "Epoch 441/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12299.1172 - val_loss: 17774.5449\n",
      "Epoch 442/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12311.0449 - val_loss: 17743.2168\n",
      "Epoch 443/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12345.8232 - val_loss: 17764.6094\n",
      "Epoch 444/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12299.7295 - val_loss: 17736.1523\n",
      "Epoch 445/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12281.4238 - val_loss: 17726.6309\n",
      "Epoch 446/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12299.1836 - val_loss: 17744.1152\n",
      "Epoch 447/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12294.5557 - val_loss: 17742.3262\n",
      "Epoch 448/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12285.2363 - val_loss: 17738.3535\n",
      "Epoch 449/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12278.9590 - val_loss: 17753.2637\n",
      "Epoch 450/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12270.8584 - val_loss: 17758.9551\n",
      "Epoch 451/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 3ms/step - loss: 12286.3779 - val_loss: 17749.5898\n",
      "Epoch 452/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12258.9756 - val_loss: 17779.1133\n",
      "Epoch 453/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12307.6689 - val_loss: 17723.6973\n",
      "Epoch 454/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12265.4199 - val_loss: 17728.5898\n",
      "Epoch 455/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12278.8164 - val_loss: 17713.5352\n",
      "Epoch 456/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12266.7139 - val_loss: 17787.7402\n",
      "Epoch 457/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12274.4365 - val_loss: 17712.5977\n",
      "Epoch 458/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12253.5234 - val_loss: 17730.4434\n",
      "Epoch 459/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12239.6406 - val_loss: 17749.3008\n",
      "Epoch 460/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12243.5430 - val_loss: 17729.0059\n",
      "Epoch 461/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12227.5840 - val_loss: 17709.3047\n",
      "Epoch 462/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12228.3350 - val_loss: 17753.5137\n",
      "Epoch 463/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12248.9785 - val_loss: 17715.7891\n",
      "Epoch 464/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12234.4170 - val_loss: 17725.7578\n",
      "Epoch 465/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12212.8906 - val_loss: 17759.2559\n",
      "Epoch 466/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12231.1660 - val_loss: 17705.9004\n",
      "Epoch 467/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12211.7520 - val_loss: 17692.5840\n",
      "Epoch 468/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12219.5498 - val_loss: 17689.1445\n",
      "Epoch 469/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12202.6904 - val_loss: 17714.5527\n",
      "Epoch 470/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12199.4297 - val_loss: 17777.8770\n",
      "Epoch 471/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12206.2256 - val_loss: 17725.7148\n",
      "Epoch 472/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12212.2881 - val_loss: 17734.2832\n",
      "Epoch 473/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12184.7100 - val_loss: 17688.2676\n",
      "Epoch 474/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12197.6562 - val_loss: 17673.4375\n",
      "Epoch 475/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12177.0654 - val_loss: 17690.1836\n",
      "Epoch 476/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12201.1797 - val_loss: 17678.6816\n",
      "Epoch 477/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12164.5352 - val_loss: 17655.9785\n",
      "Epoch 478/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12191.4580 - val_loss: 17700.7031\n",
      "Epoch 479/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12169.1924 - val_loss: 17714.4785\n",
      "Epoch 480/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12165.8604 - val_loss: 17697.3770\n",
      "Epoch 481/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12167.7031 - val_loss: 17678.9297\n",
      "Epoch 482/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12190.3379 - val_loss: 17689.6992\n",
      "Epoch 483/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12207.8838 - val_loss: 17657.5176\n",
      "Epoch 484/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12173.9043 - val_loss: 17713.9785\n",
      "Epoch 485/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12155.3857 - val_loss: 17687.8262\n",
      "Epoch 486/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12140.6914 - val_loss: 17671.4785\n",
      "Epoch 487/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12152.6191 - val_loss: 17691.4531\n",
      "Epoch 488/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12137.0439 - val_loss: 17688.4414\n",
      "Epoch 489/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12144.3408 - val_loss: 17683.4375\n",
      "Epoch 490/500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12121.1074 - val_loss: 17663.9023\n",
      "Epoch 491/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12129.9863 - val_loss: 17685.7441\n",
      "Epoch 492/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12115.9756 - val_loss: 17693.0215\n",
      "Epoch 493/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12146.2793 - val_loss: 17679.6406\n",
      "Epoch 494/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12114.4951 - val_loss: 17687.2207\n",
      "Epoch 495/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12124.9326 - val_loss: 17645.7070\n",
      "Epoch 496/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12117.3223 - val_loss: 17658.5820\n",
      "Epoch 497/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12104.8682 - val_loss: 17631.3066\n",
      "Epoch 498/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12110.9326 - val_loss: 17618.1836\n",
      "Epoch 499/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12089.1270 - val_loss: 17631.6738\n",
      "Epoch 500/500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12096.4951 - val_loss: 17652.2461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15d548c40>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=features_dict, y=labels, validation_split = 0.2, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0859c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = model.predict(test_features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c164892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('regression_solution_2.csv', 'w', newline='') as file:\n",
    "    fieldnames = ['Id', 'SalePrice']\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    n = 1461\n",
    "    for element in array:\n",
    "        writer.writerow({'Id': n, 'SalePrice': element[0]})\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae53bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef085c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
